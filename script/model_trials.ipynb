{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20571195-f1f8-4352-810d-ca9be1f3b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from proj1_helpers import *\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "from data_cleaning import *\n",
    "from data_processing import *\n",
    "from helper_functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20b938b-60c5-4748-93e1-f79f08325b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../dataset/train.csv'\n",
    "TEST_PATH = '../dataset/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0171512",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train, ids = load_csv_data(TRAIN_PATH)\n",
    "y_test , x_test, ids_test = load_csv_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ed9045",
   "metadata": {},
   "source": [
    "### DIFFERENT MODEL TRIALS USING CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb2239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cross_validation as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861fd31",
   "metadata": {},
   "source": [
    "#### 1. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276a7068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "/Users/mariatager/Desktop/ML_project1/script/data_processing.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  X_clean[:,i] = (X_clean[:,i] - np.mean(X_clean[:,i])) / np.std(X_clean[:,i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SGD: 0.574473982657946\n",
      "std:  0.05152257965744931\n"
     ]
    }
   ],
   "source": [
    "y = y_train.copy()\n",
    "tX_train, masks_train = cleaning(x_train)\n",
    "tX_test, masks_test = cleaning(x_test)\n",
    "\n",
    "# Stochastic gradient descent\n",
    "max_iters = 50\n",
    "gamma = 1e-10\n",
    "degree = 7\n",
    "k_fold = 10\n",
    "seed = 10\n",
    "\n",
    "accuracies = []\n",
    "weights_SGD = []\n",
    "total_loss = 0\n",
    "\n",
    "for i in range(len(masks_train)):\n",
    "    train_data = tX_train[masks_train[i]]\n",
    "    train_y = y[masks_train[i]]\n",
    "    test_data = tX_test[masks_test[i]]\n",
    "        \n",
    "    # Data processing\n",
    "    train_data = process_data(train_data)\n",
    "    test_data = process_data(test_data)\n",
    "\n",
    "    # Build poly\n",
    "    train_phi = build_polynomial(train_data, degree)\n",
    "    test_phi = build_polynomial(test_data, degree)\n",
    "        \n",
    "    w_init = np.zeros(train_phi.shape[1])\n",
    "    w, _ = mean_squared_error_sgd(train_y, train_phi, w_init, max_iters, gamma)\n",
    "\n",
    "    k_indices = cv.build_k_indices(train_y, k_fold, seed)\n",
    "\n",
    "    for k in range(k_fold):\n",
    "        accuracy = cv.cross_validation(train_y, train_phi, k_indices, k, mean_squared_error_sgd, initial_w = w_init, max_iters = max_iters,\n",
    "        gamma = gamma)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "print('Accuracy for SGD:', np.mean(accuracies))\n",
    "print('std: ', np.std(accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0f7c3",
   "metadata": {},
   "source": [
    "#### 2.  Ridge Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb15a2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Ridge regression is: 0.8219350855490302\n",
      "std:  0.015870927048251337\n"
     ]
    }
   ],
   "source": [
    "y = y_train.copy()\n",
    "tX_train, masks_train = cleaning(x_train)\n",
    "tX_test, masks_test = cleaning(x_test)\n",
    "\n",
    "# Parameters\n",
    "degree = 7\n",
    "lambda_ = 1e-5\n",
    "accuracies = []\n",
    "seed = 10\n",
    "k_fold = 10\n",
    "\n",
    "for i in range(len(masks_train)):\n",
    "    \n",
    "    train_data = tX_train[masks_train[i]]\n",
    "    train_y = y[masks_train[i]]\n",
    "    test_data = tX_test[masks_test[i]]\n",
    "\n",
    "    train_data = process_data(train_data)\n",
    "    test_data = process_data(test_data)\n",
    "\n",
    "    # Build poly\n",
    "    train_phi = build_polynomial(train_data, degree)\n",
    "    test_phi = build_polynomial(test_data, degree)\n",
    "    \n",
    "    # Obtain weight\n",
    "    weight, _ = ridge_regression(train_y, train_phi, lambda_)\n",
    "\n",
    "    # Compute accuracy using cross validation\n",
    "    k_indices = cv.build_k_indices(train_y, k_fold, seed)\n",
    "\n",
    "    for k in range(k_fold):\n",
    "        accuracy = cv.cross_validation(train_y, train_phi, k_indices, k, ridge_regression, lambda_=lambda_)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "print('Accuracy for Ridge regression is:', np.mean(accuracies))\n",
    "print('std: ', np.std(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20f529",
   "metadata": {},
   "source": [
    "#### 3. GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064619b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Gradient descent is: 0.6904112061048273\n",
      "std:  0.03874337772585025\n"
     ]
    }
   ],
   "source": [
    "y = y_train.copy()\n",
    "tX_train, masks_train = cleaning(x_train)\n",
    "tX_test, masks_test = cleaning(x_test)\n",
    "\n",
    "# Gradient descent parameters\n",
    "max_iters = 50\n",
    "gamma = 1e-10\n",
    "degree = 4\n",
    "seed = 10\n",
    "k_fold = 10\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for i in range(len(masks_train)):\n",
    "    train_data = tX_train[masks_train[i]]\n",
    "    train_y = y[masks_train[i]]\n",
    "    test_data = tX_test[masks_test[i]]\n",
    "        \n",
    "    # Data processing\n",
    "    train_data = process_data(train_data)\n",
    "    test_data = process_data(test_data)\n",
    "\n",
    "    # Build poly\n",
    "    train_phi = build_polynomial(train_data, degree)\n",
    "    test_phi = build_polynomial(test_data, degree)\n",
    "        \n",
    "    w_init = np.zeros(train_phi.shape[1])\n",
    "    \n",
    "    # Compute accuracy using cross validation\n",
    "    k_indices = cv.build_k_indices(train_y, k_fold, seed)\n",
    "\n",
    "    for k in range(k_fold):\n",
    "        accuracy = cv.cross_validation(train_y, train_phi, k_indices, k, mean_squared_error_gd, initial_w = w_init, max_iters = max_iters, \n",
    "        gamma = gamma)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "print('Accuracy for Gradient descent is:', np.mean(accuracies))\n",
    "print('std: ', np.std(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3e379",
   "metadata": {},
   "source": [
    "#### 4.  Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d5357b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariatager/Desktop/ML_project1/script/helper_functions.py:102: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = 1.0 / (1 + np.exp(-t))\n",
      "/Users/mariatager/Desktop/ML_project1/script/helper_functions.py:118: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y.T.dot(np.log(logistic_function)) + (1 - y).T.dot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic regression is: 0.6167690151166128\n",
      "std:  0.08127706941645539\n"
     ]
    }
   ],
   "source": [
    "y = y_train.copy()\n",
    "tX_train, masks_train = cleaning(x_train)\n",
    "tX_test, masks_test = cleaning(x_test)\n",
    "\n",
    "# Parameters\n",
    "max_iters = 50\n",
    "gamma = 1e-10\n",
    "degree = 7\n",
    "k_fold = 10\n",
    "seed = 10\n",
    "\n",
    "for i in range(len(masks_train)):\n",
    "    \n",
    "    train_data = tX_train[masks_train[i]]\n",
    "    train_y = y[masks_train[i]]\n",
    "    test_data = tX_test[masks_test[i]]\n",
    "\n",
    "    train_data = process_data(train_data)\n",
    "    test_data = process_data(test_data)\n",
    "\n",
    "    # Build poly\n",
    "    train_phi = build_polynomial(train_data, degree)\n",
    "    test_phi = build_polynomial(test_data, degree)\n",
    "\n",
    "    #Logistic regression:\n",
    "    w_init = np.zeros(train_phi.shape[1])\n",
    "\n",
    "    # Compute accuracy using cross validation\n",
    "    k_indices = cv.build_k_indices(train_y, k_fold, seed)\n",
    "\n",
    "    for k in range(k_fold):\n",
    "        accuracy = cv.cross_validation(train_y, train_phi, k_indices, k, logistic_regression, initial_w = w_init, max_iters = max_iters,\n",
    "        gamma = gamma)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "print('Accuracy for Logistic regression is:', np.mean(accuracies))\n",
    "print('std: ', np.std(accuracies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
